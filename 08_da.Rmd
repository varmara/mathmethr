---
title: "Дискриминантный анализ"
subtitle: "Математические методы в зоологии с использованием R"
author: "Марина Варфоломеева"
output:
  beamer_presentation:
    colortheme: seagull
    highlight: tango
    fonttheme: structurebold
    includes:
      in_header: ./includes/header.tex
    pandoc_args:
    - --latex-engine=xelatex
    - -V fontsize=10pt
    - -V lang=russian
    slide_level: 2
    theme: CambridgeUS
    toc: yes
---

```{r setup, include = FALSE, cache = FALSE, purl = FALSE}
options(width = 70, scipen = 6)
library(knitr)
opts_chunk$set(fig.show='hold', size='footnotesize', comment="#", warning=FALSE, message=FALSE, dev='cairo_pdf', fig.height=2.5, fig.width=7.7)
```

## Дискриминантный анализ

### Вы сможете

- провести линейный и квадратичный дискриминантный анализ с использованием обучающей выборки и проверить качество классификации на тестовых данных или с использованием кроссвалидации
- проверить условия применимости дискриминантного анализа

# Дискриминантный анализ

## Пример: Морфометрия ирисов

Сверхзадача --- научиться классифицировать ирисы по нескольким измерениям цветка

```{r}
data(iris)
head(iris, 10)
```

## По каким переменным легче всего различить группы?

Чтобы это узнать, построим графики всех пар переменных при помощи функции `pairs()` из базового пакета

```{r pairs-plot, eval=FALSE}
pairs(iris[, -5], col = iris$Species)
```

Второй вариант --- получить похожий график при помощи ggplot2, и с большим числом настроек

```{r ggpairs-plot, eval=FALSE}
library(GGally)
theme_set(theme_bw() + theme(legend.key = element_blank()))
update_geom_defaults("point", list(shape = 19, size = 2))
ggpairs(iris, aes(colour = Species, alpha = 0.5),
        upper = list(continuous = "density", combo = "box"))
```

##

```{r pairs-plot, echo=FALSE, fig.height=7, out.height='3.5in'}
```

Группы не различимы, если использовать любую из переменных отдельно, или даже какую-то пару переменных. Что делать?

##

```{r ggpairs-plot, echo=FALSE, fig.height=7, fig.width=9.8, out.height='3.5in', out.width='4.9in'}
```

## Дискриминантный анализ

\blockbegin{Дискриминантный анализ}
- метод классификации объектов с учителем (__supervised learning__), т.е. применяется, когда принадлежность объектов к группе заранее известна.
\blockend

\vfill

Задачи дискриминантного анализа:

- выяснить, какие признаки лучше всего позволяют классифицировать объекты
- выяснить правило классификации существующих объектов
- классификация новых объектов неизвестной принадлежности по этому правилу

## Требования к данным для дискриминантного анализа

- групп 2 или больше
- в каждой группе 2 и больше признаков
- число объектов должно быть больше, чем число признаков, лучше в несколько раз (в 4, например).
- признаки измерены в интервальной шкале

## Дискриминантный анализ

Нужно найти такую ось, вдоль которой группы различаются лучше всего, с минимальным перекрыванием.

Как она может проходить?

\includegraphics{images/discrim0.jpg}


## Дискриминантные оси

\blockbegin{Дискриминантные оси}

- задаются дискриминантными функциями
- вдоль них минимальное перекрывание групп
- дискриминантных осей всего на одну меньше чем групп (или столько же, сколько признаков, если признаков меньше, чем групп)

\blockend

\includegraphics{images/discrim.jpg}

## Дискриминантные функции

\blockbegin{Дискриминантные функции}

- описывают положение дискриминантных осей

\blockend

$$LD _j = d _{1j} X _{1} + d _{2i} X _{2} + ... + d _p X _{p}$$

- LD --- линейная дискриминантная функция
- d --- коэффициенты линейной дискриминантной функции
- X --- переменные-признаки
- j = 1, ... min(k--1, p) --- число дискриминантных функций
- p --- число признаков
- k --- число классов

\pause

### Стандартизованные коэффициенты дискриминантной функции

- используются для сравнения переменных, измеренных в разных шкалах используются стандартизованные коэффициенты дискриминантной функции
- большое абсолютное значение  --- большая дискриминирующая способность

## Классификация объектов

\columnsbegin
\column{0.48\textwidth}

\blockbegin{Функции классификации}

- Описывают вероятность принадлежности объекта к группе согласно построенной классификации.

- Таких функций столько же, сколько групп

\blockend


$$C _{j} = c _{j0} + c _{j1} X _{1} + ... + c _{jp} X _{p}$$

- С --- функция классификации
- с --- коэффициенты функций классификации
- X --- переменные-признаки
- j = 1, ..., k --- число групп
- p --- число признаков

\column{0.48\textwidth}

Для каждого (в том числе, нового) объекта можно вычислить значение всех функций классификации. Какое значение больше --- к такой группе и надо отнести объект.

\includegraphics{images/discrim-bound.png}

Пример расположения областей принятия решений в линейном дискриминантном анализе с тремя группами

\columnsend

\tiny{Рис. с сайта http://statweb.stanford.edu/~jtaylo/courses/stats202/lda.html}


## Оценка качества классификации

\blockbegin{Таблица классификации}
- число верно или неверно классифицированных объектов (__confusion matrix__)
\blockend

Было / Стало | Класс А | Класс Б |
-------------|---------|---------|
Класс А      | верно   | неверно (Б вместо А)|
Класс Б      | неверно (A вместо Б) | верно |

## Проблема: как оценить качество классификации, чтобы можно было экстраполировать результаты?

Если оценить качество классификации на тех же данных, что были использованы для ее построения --- оценка неадекватная для классификации новых данных из-за "__переобучения__" (overfitting).

### Возможные решения проблемы переобучения

1. Разделить данные на __тренировочное и тестовое подмножества__:
  - тренировочные данные --- для подбора классификации (для обучения)
  - независимые тестовые данные --- для определения качества классификации

2. __Кроссвалидация__ --- разделение на тренировочное и тестовое подмножество повторяют многократно и усредняют оценки качества классификации между повторами.

# I. Дискриминантный анализ на тренировочных и тестовых данных

## 1) Разделяем на тренировочные и тестовые данные

```{r}
# доля от объема выборки, которая пойдет в тренировочный датасет
smp_size <- floor(0.80 * nrow(iris))
# устанавливаем зерно для воспроизводимости результатов
set.seed(982)
# индексы строк, которые пойдут в тренировочный датасет 
in_train <- sample(sample(1:nrow(iris), size = smp_size))
```

## 2) На тренировочных данных получаем стандартизованные коэффициенты дискриминантных функций

```{r}
library(MASS)
lda_tr_scaled <- lda(scale(iris[in_train, -5]), iris$Species[in_train])
# коэффициенты дискриминантных функций
lda_tr_scaled$scaling
```



## 3) На тренировочных данных получаем функции классификации

```{r echo=FALSE, purl=TRUE}
# функция, которая добавит функций классификации к результатам дискр. анализа
lda.class <- function(x, groups){
  #   http://stackoverflow.com/q/5629550/2096842
  #   This code follows the formulas in Legendre and Legendre's Numerical Ecology (1998), page 625, and matches the results of the worked example starting on page 626.
# code slightly modified - colnames and varnames of classification functions are added
  library(MASS)
  x.lda <- lda(groups ~ ., as.data.frame(x))
  gr <- length(unique(groups))   ## groups might be factors or numeric
  v <- ncol(x) ## variables
  m <- x.lda$means ## group means
  w <- array(NA, dim = c(v, v, gr))
  for(i in 1:gr){
    tmp <- scale(subset(x, groups == unique(groups)[i]), scale = FALSE)
    w[,,i] <- t(tmp) %*% tmp
  }
  W <- w[,,1]
  for(i in 2:gr)
    W <- W + w[,,i]
  V <- W/(nrow(x) - gr)
  iV <- solve(V)
  class.funs <- matrix(NA, nrow = v + 1, ncol = gr)
  colnames(class.funs) <- unique(groups)
  rownames(class.funs) <- c("constant", colnames(x))
  for(i in 1:gr) {
    class.funs[1, i] <- -0.5 * t(m[i,]) %*% iV %*% (m[i,])
    class.funs[2:(v+1) ,i] <- iV %*% (m[i,])
  }
  x.lda$class.funs <- class.funs
  return(x.lda)
}
```
```{r}
lda_tr <- lda.class(iris[in_train, -5], iris$Species[in_train])
# Коэф. функций классификации
lda_tr$class.funs
```


## 4) Оцениваем качество классификации на тренировочных данных

```{r}
lda_tr_pred <- predict(lda_tr)
table(lda_tr_pred$class, iris$Species[in_train])
```

- Какова доля неправильно классифицированных случаев?

## 5) График классификации тренировочных данных

Один из вариантов представления --- текстом обозначить правильные группы, а цветом --- результат работы классификации

```{r}
class_df <- data.frame(lda_tr_pred$x, 
                          gr = lda_tr_pred$class, 
                          real_gr = iris$Species[in_train])
ggplot(data = class_df, aes(x = LD1, y = LD2, colour = gr)) + 
  geom_text(size = 3, aes(label = real_gr)) +
  theme(legend.position = "none")
```

## 6) Оценка качества классификации на тестовых данных

Самое важное, если мы хотим использовать классификацию для прогноза

```{r}
lda_test_pred <- predict(lda_tr, iris[-in_train, -5])
table(lda_test_pred$class, iris$Species[-in_train])
```

- Какова доля неправильно классифицированных случаев?

## 7) График классификации тестовых данных

Второй вариант представления графиков --- отметить неправильно классифицированные случаи своим цветом

```{r}
class_df <- data.frame(lda_test_pred$x, 
                          new = lda_test_pred$class, 
                          real = iris$Species[-in_train])
class_df$Group <- factor(paste(class_df$real, class_df$new, sep = " as "))

ggplot(data = class_df, aes(x = LD1, y = LD2)) + 
  geom_point(aes(colour = Group))
```

# II. Дискриминантный анализ с кроссвалидацией

## Кроссвалидация

```{r}
lda_cv <- lda(iris[, -5], iris$Species, CV = TRUE)
names(lda_cv)
table(iris$Species, lda_cv$class)
```

`lda_cv$class` --- показывает, как классифицированы строки, если классификация обучена по остальным данным

## График классификации

```{r}
ggplot(data = iris, aes(x = Petal.Length,
                        y = Sepal.Width,
                        colour = Species,
                        shape = lda_cv$class)) +
  geom_point(size = 3) +
  scale_shape_discrete("Classified as")
```


# Условия применимости дискриминантного анализа


## Условия применимости дискриминантного анализа

- __признаки независимы друг от друга__ (чтобы не было избыточности, чтобы можно было инвертировать матрицы). Именно поэтому дискр. анализ часто применяется после анализа главных компонент.
- внутригрупповые ковариации приблизительно равны
- распределение признаков --- многомерное нормальное

\pause

Если условия применимости нарушены:

- В некоторых случаях, дискриминантный анализ дает хорошо работающие классификации.

- Возможно, другие методы, с менее жесткими требованиями, дадут классификации лучшего качества (например, квадратичный дискриминантный анализ --- quadratic discriminant analysis, дискриминантный анализ с использованием ядер --- kernel discriminant analysis)


## Проверка условий применимости

В данном случае, как и во многих других, они не выполняются, но мы уже убедились, что классификация работает...


## Mногомерная нормальность

```{r}
x <- as.matrix(iris[, -5])
d <- mahalanobis(x, colMeans(x), cov(x))
qqplot(qchisq(ppoints(nrow(x)), df = ncol(x)), d,
  main="QQ график для оценки многомерной нормальности",
  ylab="Расстояние Махаланобиса")
abline(a = 0, b = 1)
```

## Гомогенность ковариационных матриц

\small

```{r}
source("BoxMTest.R")
BoxMTest(as.matrix(iris[, -5]), iris$Species)
```


# Квадратичный дискриминантный анализ


## Квадратичный дискриминантный анализ

```{r}
qda_tr <- qda(iris[in_train, -5], iris$Species[in_train])
qda_tr_pred <- predict(qda_tr)
table(qda_tr_pred$class, iris$Species[in_train])
qda_test_pred <- predict(qda_tr, iris[-in_train, -5])
table(qda_test_pred$class, iris$Species[-in_train])
```


## Take home messages

- Дискриминантный анализ --- метод классификации объектов по правилам, выработанным на выборке объектов с заранее известной принадлежностью

- Качество классификации можно оценить по числу неверно классифицированных объектов. Чтобы не было "переобучения" можно:
  - Подобрать классификацию на тренировочных данных и проверить на тестовых
  - Использовать кроссвалидацию --- классификацию объектов по правилам полученным по остальным данным (без учета этих объектов)

- Для дискриминантного анализа нужно отбирать признаки, независимые друг от друга или создавать синтетические признаки при помощи анализа главных компонент.

- Если внутригрупповые ковариации признаков различаются, лучше применять квадратичный дискриминантный анализ.


## Дополнительные ресурсы

- Quinn, Keough, 2002, pp. 435--441 

